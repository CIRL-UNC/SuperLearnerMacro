<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>superlearnerhelp</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<p><link rel="stylesheet" href="/Users/akeil/Library/Application Support/MacDown/Styles/Github2.css"></p>

<h1 id="toc_0">SuperLearnerMacro</h1>

<h3 id="toc_1">Usage details</h3>

<p>The sas script containing the SuperLearner macro actually contains 4 main macros: %SuperLearner, %_SuperLearner, %CVSuperLearner macro, and %_CVSuperLearner</p>

<h4 id="toc_2">0. Installing the macro</h4>

<h5 id="toc_3">Option 1 - run the following two lines in SAS (requires internet connection each SAS session in which super learner is used):</h5>

<div><pre><code class="language-none">FILENAME slgh URL &quot;https://cirl-unc.github.io/SuperLearnerMacro/super_learner_macro.sas&quot;;
%INCLUDE slgh;</code></pre></div>

<h5 id="toc_4">Option 2 - install from release version (requires initial internet connection):</h5>

<ol>
<li>Navigate to the <a href="https://github.com/CIRL-UNC/SuperLearnerMacro/releases">release page of the super learner macro here</a></li>
<li>Download the zip/tar.gz file to your computer and open/unzip the file  - you should see a folder called SuperLearnerMacro-XXXX, where XXXX is the release number</li>
<li>Run the following two lines in SAS (replacing appropriate path names):</li>
</ol>

<p><a href=""></a></p>

<div><pre><code class="language-none">FILENAME slgh &quot;C:/path/to/SuperLearnerMacro-XXXX/super_learner_macro.sas&quot;;
%INCLUDE slgh;</code></pre></div>

<p>Some examples of using the %SuperLearner macro are <a href="https://github.com/CIRL-UNC/SuperLearnerMacro/tree/master/examples">available here</a></p>

<h4 id="toc_5">1. Using %SuperLearner macro</h4>

<p>Stacking is based on what Wolpert refers to as a set of &lsquo;level-0&rsquo; models and a &lsquo;level-1&rsquo; model, indexed by parameters <img src="http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Cbeta%7D%5Fm" alt="equation"> and <img src="http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Calpha%7D" alt="equation"> in some study sample <em>S</em>. Where</p>

<p>Level-0: <img src="http://latex.codecogs.com/gif.latex?%5Chat%7BY%7D%5F%7Bm%7D%3Df%5Fm%28%5Cmathbf%7Bx%7D%3B%5Cmathbf%7B%5Cbeta%7D%5Fm%2CS%29%5Cmbox%7B%20for%20%7Dm%5Cin1%2C%5Cldots%2CM" alt="equation"></p>

<p>Level-1: <img src="http://latex.codecogs.com/gif.latex?%5Chat%7BY%7D%5F%7Bsl%7D%3Df%5F%7Bsl%7D%28%5Chat%7B%5Cmathbf%7BY%7D%7D%5F%7B%5Cbar%7Bm%7D%7D%3B%5Cmathbf%7B%5Calpha%7D%2CS%29" alt="equation"></p>

<p>The parameterization of the macro is based loosely on this notation. Each level-0 model is referred to as a &lsquo;learner&rsquo; in the super learner library. A call to super learner is structured as follows:</p>

<div><pre><code class="language-none">%SuperLearner(
 Y=,
 X=,
 library=, 
 indata=, 
 preddata=, 
 outdata=sl_out,
 dist=GAUSSIAN,
 method=NNLS
 by=,
 intvars=,
 binary_predictors=,
 ordinal_predictors=,
 nominal_predictors=,
 continuous_predictors=,
 weight=, 
 trtstrat=false, 
 folds=10 
);</code></pre></div>

<p>Macro parameters include the following:</p>

<ul>
<li><p><strong>Y</strong>: [value = variable name] the target variable, or outcome</p></li>
<li><p><strong>X</strong>: [value =   blank, or a space separated list of variable names] predictors of <strong>Y</strong> on the right side of the level-0 models. Note that this is a convenience function for the individual <strong>[coding]_predictors</strong> macro variables. The macro will make a guess at whether each predictor in <strong>X</strong> is continuous, categorical, or binary. (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> - <strong>binary_predictors</strong>, <strong>ordinal_predictors</strong>, <strong>nominal_predictors</strong>, <strong>continuous_predictors</strong> - parameters must be specified, as described below). If <strong>X</strong> is specified and any one of the <strong>[coding]_predictors</strong> has a value, the macro will generate an error.</p></li>
<li><p><strong>library</strong>: [value =  a space separated list of learners] the names of the <em>m</em> level-0 models (e.g. glm lasso cart). A single learner can be used here if you only wish to know the cross-validated expected loss (e.g. mean-squared error). See <a href="availablelearners.html" title="Available learners">all available default learners here</a> and <a href="newlearners.html" title="Custom learners">how to construct new learners here</a></p></li>
<li><p><strong>indata</strong>: [value = an existing dataset name] the dataset used for analysis that contains <em>Y</em> and all predictors (and weight variables, if needed)</p></li>
<li><p><strong>preddata</strong>: [OPTIONAL value = a dataset name] the validation dataset. A dataset which contains all predictors and possibly <em>Y</em> that is not used in model fitting but predictions for each learner and superlearner are made in these data</p></li>
<li><p><strong>outdata</strong>: [value = a dataset name; default: sl_out] an output dataset that will contain all predictions as well as all variables and observations in the <strong>indata</strong> and <strong>preddata</strong> datasets</p></li>
<li><p><strong>dist</strong>: [value = one of: GAUSSIAN,BERNOULLI; default GAUSSIAN] Super learner can be used to make predictions of a continuous (assumed gaussian in some learners) or a binary variable. Use GAUSSIAN for all continuous variables and BERNOULLI for all binary variables. Nominal/categorical variables currently not supported.</p></li>
<li><p><strong>method</strong>:[value = one of: NNLS, CCNLS, OLS, NNLOGLIK, CCLOGLIK, LOGLIK, CCLAE, NNLAE, LAE, CCRIDGE, NNRIDGE, RIDGE, BNNLS, BCCNLS, BOLS, BNNLOGLIK, BCCLOGLIK, BLOGLIK, BCCLAE, BNNLAE, BLAE, BCCRIDGE, BNNRIDGE, BRIDGE, BCCLASSO, BNNLASSO, BLASSO; default NNLS] the method used to estimate the <img src="http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Calpha%7D" alt="equation"> coefficients of the level-1 model.                    Methods are indexed by [prefix][suffix], where the prefix sets constraints and the suffix sets the mean model form</p>

<ul>
<li>prefixes: NN, CC, BNN, BCC, B, [none], where 

<ul>
<li>B implies &ldquo;big&rdquo; and combines with other prefixes- non-B methods are fit via optimization in the OPTMODEL procedure. This method is sufficient and robust for many problems, but OPTMODEL may fail due to memory limitations with large datasets. The &ldquo;B&rdquo; methods are fit via the HPNLMODEL procedure, which does not have the same memory constraints, but may be less robust for routine use. If you get an error regarding running out of memory when running the macro or are using training data with &gt; 500k observations, try switching to a &ldquo;B&rdquo; version of the method (e.g. BNNLS rather than NNLS). B methods may eventually become the default if robustness is established. In some difficult models (e.g. with method=LAE), B methods may be faster.</li>
<li>NN implies non-negative coefficients that are standardized after fitting to sum to 1. </li>
<li>CC implies a convexity constraint where the super learner fit is subject to a constraint that forces the coefficients to fall in [0,1] and sum to 1.0. No prefix implies no constraints (which results in some loss of asymptotic properties such as the oracle property). Note: OLS violates this naming convention, but LS will also be accepted and is equivalent to OLS</li>
</ul></li>
<li>suffixes: LS, RIDGE, LOGLIK, LAE, LASSO, where

<ul>
<li>LS methods use an L2 loss function (least squares - OLS, NNLS, CCLS, BOLS, BNNLS, BCCLS). This method selects the super learner fit by minimizing the (cross-validated) mean-squared error (GAUSSIAN distribution) or the Brier score (BERNOULLI distribution). </li>
<li>RIDGE methods use an L2 penalized L2 loss function (ridge regression - RIDGE, NNRIDGE, CCRIDGE, BRIDGE, BNNRIDGE, BCCRIDGE). Penalization level can be modified in the %_SuperLearner macro via the <strong>slridgepen</strong> parameter (default 0.3).</li>
<li>LOGLIK methods use a loss function corresponding to the binomial likelihood with a logit link function (logistic regression - LOGLIK, NNLOGLIK, CCLOGLIK, BLOGLIK, BNNLOGLIK, BCCLOGLIK)</li>
<li>LAE methods [experimental] use an L1 loss function (least absolute error), which will not penalize outliers as much as L2 methods, and is also non-differentiable at the minimum (median regression - LAE, NNLAE, CCLAE, BLAE, BNNLAE, BCCLAE) which may cause computational difficulties</li>
<li>LASSO methods use an L1 penalized L2 loss function (LASSO regression - B methods only: BLASSO, BNNLASSO, BCCLASSO). Penalization level can be modified in the %_SuperLearner macro via the <strong>slridgepen</strong> parameter (default 0.3).</li>
</ul></li>
</ul></li>
<li><p><strong>by</strong>: [OPTIONAL value = variable name] a by variable in the usual SAS usage. Separate super learner fits will be specified for each level of the by variable (only one allowed, unlike typical &ldquo;by&rdquo; variables. </p></li>
<li><p><strong>intvars</strong>:[OPTIONAL value = variable name] an intervention variable that is included in the list of predictors. This is a convenience function that will make separate predictions for the intvars variable at 1 or 0 (with all other predictors remaining at their observed levels)</p></li>
<li><p><strong>binary_predictors</strong>: [value =  blank, or a space separated list of variable names] advanced specification of predictors: a space separated list of binary predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>ordinal_predictors</strong>: [value =  blank, or a space separated list of variable names]advanced specification of predictors: a space separated list of ordinal predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>nominal_predictors</strong>: [value =  blank, or a space separated list of variable names]advanced specification of predictors: a space separated list of nominal predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>continuous_predictors</strong>: [value =  blank, or a space separated list of variable names] advanced specification of predictors: a space separated list of continuous predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>weight</strong>: [OPTIONAL value = a variable name] a variable containing weights representing the relative contribution of each observation to the fit (a.k.a. case weights). Not all learners will respect non-integer weights, so weights will either be ignored or truncated by some procedures.</p></li>
<li><p><strong>trtstrat</strong>: [value  = true, false; DEFAULT: false] convenience function. If this is set to true and <strong>intvars</strong> is specified, then all fits will be stratified by levels of <strong>intvars</strong>. Levels 0,1 only.</p></li>
<li><p><strong>folds</strong>: [value  = integer ; default: 10] number of cross-validation folds to use.</p></li>
</ul>

<h4 id="toc_6">2. %_SuperLearner macro</h4>

<p>This is a version of the %SuperLearner macro for advanced users that may be somewhat faster due to reduced error checking, and offers finer level controls. If the %SuperLearner macro completes successfully, it will give some example code that can be run with %_SuperLearner. Of note, there is no checking or correction of parameter syntax, so the case-sensitive parameter arguments may cause an error in %_SuperLearner, but not %SuperLearner.</p>

<p>One main difference is that %_SuperLearner will make no guesses about variable types for <strong>X</strong>, so use of the <strong>[coding]_predictors</strong> parameters is required for correct specification. See the source code for documentation of additional options.</p>

<h4 id="toc_7">3. %CVSuperLearner macro</h4>

<p>This macro is used to estimate the cross-validated expected loss of super learner itself. It does not produce predictions! This gives an idea about whether super learner is the appropriate learner to use in a given scenario, and allows some choice between parameters of the the super learner model, such as the method (e.g. NNLS vs. CCLS).</p>

<ul>
<li><strong>folds</strong>:[value = integer; default: 10] specifies two quantities (which can be individually specified in the %_CVSuperLearner macro):

<ol>
<li><strong>slfolds</strong> number of &ldquo;inner folds&rdquo; (number of folds within each super learner fit) should only be different from <strong>cvslfolds</strong> in odd cases</li>
<li> <strong>cvslfolds</strong>: number of &ldquo;outer folds&rdquo; (the number of folds for cross-validating super learner) should only be different from <strong>slfolds</strong> in odd cases</li>
</ol></li>
</ul>

<p>Options repeated from %SuperLearner (see definitions given above)</p>

<p><strong>Y</strong>, <strong>X</strong>, <strong>binary_predictors</strong>, <strong>ordinal_predictors</strong>, <strong>nominal_predictors</strong>, <strong>continuous_predictors</strong>, <strong>weight</strong>, <strong>indata</strong>, <strong>dist</strong>, <strong>library</strong>, <strong>method</strong></p>

<h4 id="toc_8">4. %_CVSuperLearner macro</h4>

<p>This is a version of the %CVSuperLearner macro for advanced users that may be somewhat faster due to reduced error checking, and offers finer level controls. See the source code for further tuning options.</p>

<h4 id="toc_9">Getting errors?</h4>

<p>See the <a href="https://cirl-unc.github.io/SuperLearnerMacro/docs/TroubleShooting.html" title="Trouble shooting">Troubleshooting help</a></p>

<h4 id="toc_10">Further reading</h4>

<h5 id="toc_11">About this macro</h5>

<ol>
<li>A. P. Keil. Super Learning in the SAS system. ArXiv e-prints, May 2018. <a href="https://arxiv.org/abs/1805.08058">https://arxiv.org/abs/1805.08058</a></li>
</ol>

<h5 id="toc_12">About stacking</h5>

<ol>
<li><p>D. H. Wolpert. Stacked generalization. Neural networks, 5(2):241–259, 1992.</p></li>
<li><p>L. Breiman. Stacked regressions. Machine learning, 24(1):49–64, 1996.</p></li>
</ol>

<h5 id="toc_13">About super learner</h5>

<ol>
<li><p>M. J. van der Laan, E. C. Polley, and A. E. Hubbard. Super learner. Report, Division of Biostatistics, University of California, Berkeley, 2007.</p></li>
<li><p>E. C. Polley and M. J. van der Laan. Super learner in prediction. Report, Division of Biostatistics, University of California, Berkeley, 2010.</p></li>
</ol>

<h4 id="toc_14">Acknowledgements</h4>

<p>This work was only possible with valuable advice and beta testing from the following people: Stephen R Cole, Jessie K Edwards, Katie M O&#39;Brien, Eric Polley, Marie Stoner, Jennifer Winston and many others</p>

<p><strong><a href="https://cirl-unc.github.io/SuperLearnerMacro" title="Home">Super learner macro home page</a></strong></p>



<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
