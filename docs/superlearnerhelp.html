<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>superlearnerhelp</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h1 id="toc_0">SuperLearnerMacro</h1>

<h3 id="toc_1">Usage details</h3>

<p>The sas script containing the SuperLearner macro actually contains 4 main macros: %SuperLearner, %_SuperLearner, %CVSuperLearner macro, and %_CVSuperLearner</p>

<h4 id="toc_2">0. Installing the macro</h4>

<h5 id="toc_3">Option 1 - run the following two lines in SAS (requires internet connection each SAS session in which super learner is used):</h5>

<div><pre><code class="language-none">FILENAME slgh URL &quot;https://raw.githubusercontent.com/CIRL-UNC/SuperLearnerMacro/9c7b712b074cda44a9a3acbb7a8b25bba32aab1e/super_learner_macro.sas&quot;;
%INCLUDE slgh;</code></pre></div>

<h5 id="toc_4">Option 2 - install from release version (requires initial internet connection):</h5>

<ol>
<li>Navigate to the <a href="https://github.com/CIRL-UNC/SuperLearnerMacro/releases">release page of the super learner macro here</a></li>
<li>Download the zip/tar.gz file to your computer and open/unzip the file  - you should see a folder called SuperLearnerMacro-XXXX, where XXXX is the release number</li>
<li>Run the following two lines in SAS (replacing appropriate path names):</li>
</ol>

<p><a href=""></a></p>

<div><pre><code class="language-none">FILENAME slgh &quot;C:/path/to/SuperLearnerMacro-XXXX/super_learner_macro.sas&quot;;
%INCLUDE slgh;</code></pre></div>

<p>Some examples of using the %SuperLearner macro are <a href="https://github.com/CIRL-UNC/SuperLearnerMacro/tree/master/examples">available here</a></p>

<h4 id="toc_5">1. Using %SuperLearner macro</h4>

<p>Stacking is based on what Wolpert refers to as a set of &#39;level-0&#39; models and a &#39;level-1&#39; model, indexed by parameters <img src="http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Cbeta%7D%5Fm" alt="equation"> and <img src="http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Calpha%7D" alt="equation"> in some study sample <em>S</em>. Where</p>

<p>Level-0: <img src="http://latex.codecogs.com/gif.latex?%5Chat%7BY%7D%5F%7Bm%7D%3Df%5Fm%28%5Cmathbf%7Bx%7D%3B%5Cmathbf%7B%5Cbeta%7D%5Fm%2CS%29%5Cmbox%7B%20for%20%7Dm%5Cin1%2C%5Cldots%2CM" alt="equation"></p>

<p>Level-1: <img src="http://latex.codecogs.com/gif.latex?%5Chat%7BY%7D%5F%7Bsl%7D%3Df%5F%7Bsl%7D%28%5Chat%7B%5Cmathbf%7BY%7D%7D%5F%7B%5Cbar%7Bm%7D%7D%3B%5Cmathbf%7B%5Calpha%7D%2CS%29" alt="equation"></p>

<p>The parameterization of the macro is based loosely on this notation. A call to super learner is structured as follows:</p>

<div><pre><code class="language-none">%SuperLearner(
 Y=,
 X=,
 library=, 
 indata=, 
 preddata=, 
 outdata=sl_out,
 dist=GAUSSIAN,
 method=NNLS
 by=,
 intvars=,
 binary_predictors=,
 ordinal_predictors=,
 nominal_predictors=,
 continuous_predictors=,
 weight=, 
 trtstrat=false, 
 folds=10, 
);</code></pre></div>

<p>Macro parameters include the following:</p>

<ul>
<li><p><strong>Y</strong>: [value = variable name] the target variable, or outcome</p></li>
<li><p><strong>X</strong>: [value =   blank, or a space separated list of variable names] predictors of <strong>Y</strong> on the right side of the level-0 models. Note that this is a convenience function for the individual <strong>[coding]_predictors</strong> macro variables. The macro will make a guess at whether each predictor in <strong>X</strong> is continuous, categorical, or binary. (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> - <strong>binary_predictors</strong>, <strong>ordinal_predictors</strong>, <strong>nominal_predictors</strong>, <strong>continuous_predictors</strong> - parameters must be specified, as described below). If <strong>X</strong> is specified and any one of the <strong>[coding]_predictors</strong> has a value, the macro will generate an error.</p></li>
<li><p><strong>library</strong>: [value =  a space separated list of learners] the names of the <em>m</em> level-0 models (e.g. glm lasso cart). A single learner can be used here if you only wish to know the cross-validated expected loss (e.g. mean-squared error). See <a href="availablelearners.html" title="Available learners">all available default learners here</a> and <a href="newlearners.html" title="Custom learners">how to construct new learners here</a></p></li>
<li><p><strong>indata</strong>: [value = an existing dataset name] the dataset used for analysis that contains <em>Y</em> and all predictors (and weight variables, if needed)</p></li>
<li><p><strong>preddata</strong>: [OPTIONAL value = a dataset name] the validation dataset. A dataset which contains all predictors and possibly <em>Y</em> that is not used in model fitting but predictions for each learner and superlearner are made in these data</p></li>
<li><p><strong>outdata</strong>: [value = a dataset name; default: sl_out] an output dataset that will contain all predictions as well as all variables and observations in the <strong>indata</strong> and <strong>preddata</strong> datasets</p></li>
<li><p><strong>dist</strong>: [value = one of: GAUSSIAN,BERNOULLI; default GAUSSIAN] Super learner can be used to make predictions of a continuous (assumed gaussian in some learners) or a binary variable. Use GAUSSIAN for all continuous variables and BERNOULLI for all binary variables. Nominal/categorical variables currently not supported.</p></li>
<li><p><strong>method</strong>:[value = one of: NNLS,NNLOGLIK,CCLOGLIK,LOGLIK,NNLS,OLS,CCLAE,NNLAE,LAE; default NNLS] the method used to estimate the <img src="http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Calpha%7D" alt="equation"> coefficients of the level-1 model.                    Methods are possibly indexed by prefixes: NN, CC, [none], where </p>

<p>NN implies non-negative coefficients that are standardized after fitting to sum to 1. </p>

<p>CC implies a convexity constraint where the super learner fit is subject to a constraint 
that forces the coefficients to fall in [0,1] and sum to 1.0. No prefix implies no 
constraints (which results in some loss of asymptotic properties such as the oracle property).
Note: OLS violates this naming convention, but LS will also be accepted and is equivalent to OLS</p>

<p>LS methods use an L2 loss function (least squares)
LOGLIK methods use a loss function corresponding to the binomial likelihood with a logit link function
LAE methods [experimental] use an L1 loss function (least absolute error), which will not penalize outliers as much as L2 methods, and is also non-differentiable at the minimum
which may cause computational difficulties</p></li>
<li><p><strong>by</strong>: [OPTIONAL value = variable name] a by variable in the usual SAS usage. Separate super learner fits will be specified for each level of the by variable (only one allowed, unlike typical ``by&#39;&#39; variables. </p></li>
<li><p><strong>intvars</strong>:[OPTIONAL value = variable name] an intervention variable that is included in the list of predictors. This is a convenience function that will make separate predictions for the intvars variable at 1 or 0 (with all other predictors remaining at their observed levels)</p></li>
<li><p><strong>binary_predictors</strong>: [value =  blank, or a space separated list of variable names] advanced specification of predictors: a space separated list of binary predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>ordinal_predictors</strong>: [value =  blank, or a space separated list of variable names]advanced specification of predictors: a space separated list of ordinal predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>nominal_predictors</strong>: [value =  blank, or a space separated list of variable names]advanced specification of predictors: a space separated list of nominal predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>continuous_predictors</strong>: [value =  blank, or a space separated list of variable names] advanced specification of predictors: a space separated list of continuous predictors (OPTIONAL but at least one of the <strong>X</strong> or <strong>[coding]_predictors</strong> parameters must be specified)</p></li>
<li><p><strong>weight</strong>: [OPTIONAL value = a variable name] a variable containing weights representing the relative contribution of each observation to the fit (a.k.a. case weights). Not all learners will respect non-integer weights, so weights will either be ignored or truncated by some procedures.</p></li>
<li><p><strong>trtstrat</strong>: [value  = true, false; DEFAULT: false] convenience function. If this is set to true and <strong>intvars</strong> is specified, then all fits will be stratified by levels of <strong>intvars</strong> (0,1) accepted only.</p></li>
<li><p><strong>folds</strong>: [value  = integer ; default: 10] number of cross-validation folds to use</p></li>
</ul>

<h4 id="toc_6">2. %_SuperLearner macro</h4>

<p>This is a less user-friendly version of the %SuperLearner macro that may be somewhat faster due to reduced error checking, and offers finer level controls. If the %SuperLearner macro completes successfully, it will give some example code that can be run with %_SuperLearner. Of note, there is no checking of parameter syntax, so the case-sensitive parameter arguments may cause an error in %_SuperLearner, but not %SuperLearner.</p>

<p>One main difference is that %_SuperLearner will make no guesses about variable types for <strong>X</strong>, so use of the <strong>[coding]_predictors</strong> is required for correct specification.</p>

<h4 id="toc_7">3. %CVSuperLearner macro</h4>

<p>This macro is used to estimate the cross-validated expected loss of super learner itself. It does not produce predictions! This gives an idea about whether super learner is the appropriate learner to use in a given scenario, and allows some choice between parameters of the the super learner model, such as the method (e.g. NNLS vs. CCLS).</p>

<ul>
<li><strong>folds</strong>:[value = integer; default: 10] specifies two quantities (which can be individually specified in the %_CVSuperLearner macro):

<ol>
<li><strong>slfolds</strong> number of &#39;&#39;inner folds&#39;&#39; (number of folds within each super learner fit) should only be different from <strong>cvslfolds</strong> in odd cases</li>
<li> <strong>cvslfolds</strong>: number of &#39;&#39;outer folds&#39;&#39; (the number of folds for cross-validating super learner) should only be different from <strong>slfolds</strong> in odd cases</li>
</ol></li>
</ul>

<p>Options repeated from %SuperLearner</p>

<ul>
<li><strong>Y</strong>: see %SuperLearner macro definition</li>
<li><strong>X</strong>: see %SuperLearner macro definition</li>
<li><strong>by</strong>: see %SuperLearner macro definition</li>
<li><strong>binary_predictors</strong>:  see %SuperLearner macro definition</li>
<li><strong>ordinal_predictors</strong>:  see %SuperLearner macro definition</li>
<li><strong>nominal_predictors</strong>: see %SuperLearner macro definition </li>
<li><strong>continuous_predictors</strong>:  see %SuperLearner macro definition</li>
<li><strong>weight</strong>: see %SuperLearner macro definition</li>
<li><strong>indata</strong>: see %SuperLearner macro definition</li>
<li><strong>dist</strong>: see %SuperLearner macro definition (default: GAUSSIAN)</li>
<li><strong>library</strong>:  see %SuperLearner macro definition </li>
<li><strong>method</strong>:  see %SuperLearner macro definition (default: NNLS)</li>
</ul>

<h4 id="toc_8">4. %_CVSuperLearner macro</h4>

<p>This is a less user-friendly version of the %CVSuperLearner macro that may be somewhat faster due to reduced error checking, and offers finer level controls. See the source code for further tuning options.</p>

<h4 id="toc_9">Further reading</h4>

<h5 id="toc_10">About this macro</h5>

<ol>
<li>A. P. Keil. Super Learning in the SAS system. ArXiv e-prints, May 2018. <a href="https://arxiv.org/abs/1805.08058">https://arxiv.org/abs/1805.08058</a></li>
</ol>

<h5 id="toc_11">About stacking</h5>

<ol>
<li><p>D. H. Wolpert. Stacked generalization. Neural networks, 5(2):241–259, 1992.</p></li>
<li><p>L. Breiman. Stacked regressions. Machine learning, 24(1):49–64, 1996.</p></li>
</ol>

<h5 id="toc_12">About super learner</h5>

<ol>
<li><p>M. J. van der Laan, E. C. Polley, and A. E. Hubbard. Super learner. Report, Division of Biostatistics, University of California, Berkeley, 2007.</p></li>
<li><p>E. C. Polley and M. J. van der Laan. Super learner in prediction. Report, Division of Biostatistics, University of California, Berkeley, 2010.</p></li>
</ol>

<h4 id="toc_13">Acknowledgements</h4>

<p>This work would not have been possible without valuable advice and beta testing from the following people: Jessie K Edwards, Katie M O&#39;Brien, Stephen R Cole, and many others</p>

<p><strong><a href="https://cirl-unc.github.io/SuperLearnerMacro" title="Home">Super learner macro home page</a></strong></p>



<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
